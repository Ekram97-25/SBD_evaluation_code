# -*- coding: utf-8 -*-
"""SBD_Final_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19urgPOcO3T0hwR3xxzKwHJ9EpH31A4V9
"""

# In Google colab the pips had to keep being rerun,
#that is why they're in the code.

pip install wtpsplit #25s

pip install -U SoMaJo #8s

pip install stanza #2m

pip install --user -U nltk #8s

pip install -U spacy #9s

#https://spacy.io/models/de#de_core_news_sm
!python -m spacy download de_core_news_sm #28s

#import all the needed libraries
import pandas as pd
import glob
import wtpsplit
import stanza
import nltk
import spacy
import matplotlib.pyplot as plt
from wtpsplit import SaT
from somajo import SoMaJo
from nltk.tokenize import sent_tokenize #41s

#implemetn the needed sentence splitter model
#https://github.com/segment-any-text/wtpsplit?tab=readme-ov-file
sat = SaT('sat-12l-sm') #the best wtpsplit model, and there's also 'sat-3l-sm'
sat.half().to("cpu") #'cpu' was used, because my laptop does not have Nvidia GPU

#https://github.com/tsproisl/SoMaJo?tab=readme-ov-file
#load the German CMC model, and enable splitting of camel cases
tokenizer = SoMaJo("de_CMC", split_camel_case=True)

#https://stanfordnlp.github.io/stanza/tokenize.html#tokenization-and-sentence-segmentation
#from the stanza pipeline use the tokenize processor and the German language
stanza_nlp = stanza.Pipeline(lang='de', processors='tokenize')

#https://guides.library.upenn.edu/penntdm/python/nltk#:~:text=In%20NLTK%2C%20you%20can%20use,given%20input%20text%20into%20sentences.
#nltk.download('punkt')
nltk.download('punkt_tab') #the new pickle-free 'punkt'

#https://spacy.io/models/de#de_core_news_sm
spacy.prefer_gpu() #checks if the prefered GPU of spaCy is available
#German pipeline optimized for CPU;
spacy_nlp = spacy.load('de_core_news_sm', exclude=["parser"])

def read_text_files(path):
  """Using the glob library, all txt files from the given directory
  get opened and cleaned from newlines and unnecessary spaces before
  periods, then saved into a list.
  The function returns a string of the saved list, for easier use in all
  tools, other than wtpsplit"""

    files = glob.glob(path)
    texts_list = []
    for file in files:
        with open(file, 'r') as f:
            lines = f.readlines()
            clean_txt = ' '.join(line.strip().replace(' .', '.') for line in lines) #line.strip().replace(' .', '.') for line in lines
            texts_list.append(clean_txt)
    return ' '.join(texts_list)

def wtpsplit_segmentation(text):
  """Inputs the string of all the texts inot wtpsplit's model.
  The counter is to have a visual of how many sentences were segmented. """
  result = sat.split(text)
  counter = 0
  for sent in result:
    counter +=1
  print(f"wtpsplit segmented {counter} sentences.")
  return result, counter

def somajo_segmentation(text_list):
  """Puts the text List into the tokenizer module, than it iterates through all
  the sentences and while counting the final sentence number, it goes thought
  each token in each sentence, and appends it all in a dictionary in the result
  list. enumerate() was used to be able to find out the number of sentences."""
  #tokenizer = SoMaJo("de_CMC", split_camel_case=True)
  sentences = tokenizer.tokenize_text(text_list)
  result = []
  counter = 0
  for i, sentence in enumerate(sentences):
    counter += 1
    for token in sentence:
      result.append({'sent_num': i+1, 'word': token.text})
  print(f"SoMaJo segmented {counter} sentences.")
  return result, counter

def stanza_segmentation(text):
  """Using the nlp(text) stored in doc, it iterates througheach sentence
  and gives the final result out in a list of dictionaries, were for each
  sent_num the words (token.text) in that sentence are added.
  Because the dataset is over 1M characters long, the max_length was
  adjusted with an added buffer of a 1000 to be extra cautios"""
  #stanza_nlp = stanza.Pipeline(lang='de', processors='tokenize')
  stanza_nlp.max_length = len(text) + 1000
  doc = stanza_nlp(text)
  result = []
  counter = 0
  for i, sentence in enumerate(doc.sentences):
    counter += 1
    for token in sentence.tokens:
      result.append({'sent_num': i+1, 'word': token.text})
  print(f"Stanza segmented {counter} sentences.")
  return result, counter

def nltk_segmentation(text):
  """ Inputting the text string into the nltk sentence tokenizer, that then
  iterates throught each sentence and segmetns them, which than are appended
  to the results list"""
  #nltk.download('punkt_tab')
  sentences = sent_tokenize(text)
  result = []
  counter = 0
  for sentence in sentences:
    counter += 1
    result.append(sentence)
  print(f"NLTK segmented {counter} sentences.")
  return result, counter

def spacy_segmentation(text):
  """German pipeline optimized for CPU; this function uses its component
  senter to segment each text into the sentences that are returned.
  Because the dataset is over 1M characters long, the max_length was
  adjusted with an added buffer of a 1000 to be extra cautios"""
  #spacy.prefer_gpu()
  #spacy_nlp = spacy.load('de_core_news_sm', exclude=["parser"])
  spacy_nlp.enable_pipe("senter")
  spacy_nlp.max_length = len(text) + 1000
  doc = spacy_nlp(text)
  result = []
  counter = 0
  for sent in doc.sents:
    counter += 1
    result.append(sent.text)
  print(f"SpaCy segmented {counter} sentences.")
  return result, counter

def save_to_excel(results, filename):
  """turning every result list into an Excel sheet"""
    df = pd.DataFrame(results)
    df.to_excel(filename, index=False)

def plot_segmentation_counts(seg_counts):
  """Plots a bar chart to compare the number of segmented sentences produced by
    the different segmentation tools.
    Parameters:
    seg_counts (dict): A dictionary where keys are segmentation tool names,
    and values are the number of segmented sentences, and each tool gets a
    color.

    Returns:None
    """

  plt.figure(figsize=(10, 6))
  plt.bar(seg_counts.keys(), seg_counts.values(), color=['blue', 'orange', 'green', 'red', 'yellow'])
  plt.xlabel("Segmentation Tools")
  plt.ylabel("Number of Segmented Sentences")
  plt.title("Sentence Segmentation Comparison")
  plt.xticks(rotation=15)

  for i, v in enumerate(seg_counts.values()):
    plt.text(i, v + 50, str(v), ha='center', fontsize=12, fontweight='bold')
  plt.show()

def main():
  """Every functions inputs are called here.
  Before use, change the Path to the path on your device"""
  text = read_text_files('/content/drive/MyDrive/Colab_Notebooks/litkey_data/manually_seg/*.txt')

  #dictionary of the tools functions
  tools = {
      "wtpsplit": wtpsplit_segmentation(text),
      "SoMaJo": somajo_segmentation([text]),
      "Stanza": stanza_segmentation(text),
      "NLTK": nltk_segmentation(text),
      "SpaCy": spacy_segmentation(text)
      }
  #create a key value for each tools counter, by iterating throught
  #each tool and counter for all the tools
  seg_counts = {tool: counter for tool, (_, counter) in tools.items()}

  #iterate through the results and tools for all the tools,
  #and save the results into an Excel sheet
  for tool, (result,_) in tools.items():
    save_to_excel(result, f"{tool}_segmentation.xlsx")

  plot_segmentation_counts(seg_counts)

#runs the program
if __name__ == "__main__":
    main()
#31 minutes, manually segmented 50 texts, all tools
#31 minutes, original 50 texts, all tools
#2 minutes, all 1922 texts, without the wtpsplit tool